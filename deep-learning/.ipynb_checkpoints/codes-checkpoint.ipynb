{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Plots\n",
    "actual(x) = exp(-x^2)\n",
    "x_train, x_test = hcat(0:5...), hcat(6:10...)\n",
    "y_train, y_test = actual.(x_train), actual.(x_test)\n",
    "\n",
    "# introduce some noise in training data \n",
    "y_train = y_train + randn(size(y_train))\n",
    "\n",
    "\n",
    "# define loss function \n",
    "using Statistics\n",
    "loss(model, x, y) = mean(abs2.(model(x) .- y));\n",
    "\n",
    "using Flux: train!\n",
    "# Gradient Descent optimizer\n",
    "opt = Descent()\n",
    "data = [(x_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n================ Q1: Linear Regression ================\")\n",
    "# two parameters basic linear regression \n",
    "predict = Dense(1 => 1)\n",
    "println(\"Basic Linear regression \")\n",
    "for epoch in 1:20\n",
    "   train!(loss, predict, data, opt)\n",
    "   @show loss(predict, x_train, y_train)\n",
    "end\n",
    "println(\"=== Basic Linear Regression complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for Q1\n",
    "x_plot = collect(0:0.1:10)\n",
    "y_true = actual.(x_plot)\n",
    "y_pred_lr = predict(hcat(x_plot...))\n",
    "\n",
    "plot(x_plot, y_true, label=\"True Function\", lw=2)\n",
    "scatter!(vec(x_train), vec(y_train), label=\"Training Data\")\n",
    "plot!(x_plot, vec(y_pred_lr), label=\"Linear Regression Prediction\", lw=2)\n",
    "savefig(\"Q1_linear_regression.png\")\n",
    "println(\"reset the data\")\n",
    "data = [(x_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n================ Q2: Feedforward Neural Network ================\")\n",
    "# activation function relu - trying multiple configurations\n",
    "println(\"Activation function : relu (multiple configurations)\")\n",
    "\n",
    "configs = [10, 20, 50]  # different hidden layer sizes\n",
    "\n",
    "for n_hidden in configs\n",
    "    println(\"\\nTraining model with \", n_hidden, \" hidden neurons\")\n",
    "    \n",
    "    model_relu = Chain(\n",
    "        Dense(1 => n_hidden, relu),\n",
    "        Dense(n_hidden => 1)\n",
    "    )\n",
    "    \n",
    "    opt_relu = Descent()  # fresh optimizer for each model\n",
    "    \n",
    "    for epoch in 1:20\n",
    "        train!(loss, model_relu, data, opt_relu)\n",
    "    end\n",
    "    \n",
    "    println(\"Final training loss: \", loss(model_relu, x_train, y_train))\n",
    "    y_pred_nn = model_relu(hcat(x_plot...))\n",
    "    plot(x_plot, y_true, label=\"True Function\", lw=2)\n",
    "    scatter!(vec(x_train), vec(y_train), label=\"Training Data\")\n",
    "    plot!(x_plot, vec(y_pred_nn), label=\"NN with $(n_hidden) neurons\", lw=2)\n",
    "    savefig(\"Q2_nn_$(n_hidden).png\")\n",
    "end\n",
    "\n",
    "println(\"complete , resetting the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n================ Q3: Recurrent Neural Network ================\")\n",
    "# Recurrent Neural Network experiments\n",
    "println(\"Recurrent Neural Network (RNN) configurations\")\n",
    "\n",
    "rnn_configs = [5, 10, 20]  # number of hidden units\n",
    "\n",
    "# reshape data for RNN: (features, sequence_length, batch_size)\n",
    "x_train_rnn = reshape(x_train, 1, :, 1)\n",
    "y_train_rnn = reshape(y_train, 1, :, 1)\n",
    "\n",
    "for hidden_size in rnn_configs\n",
    "    println(\"\\nTraining RNN with \", hidden_size, \" hidden units\")\n",
    "    \n",
    "    model_rnn = Chain(\n",
    "        RNN(1 => hidden_size, tanh),\n",
    "        Dense(hidden_size => 1)\n",
    "    )\n",
    "    \n",
    "    opt_rnn = Descent()\n",
    "    \n",
    "    for epoch in 1:30\n",
    "        Flux.reset!(model_rnn)\n",
    "        train!( (m, x, y) -> mean(abs2.(m(x) .- y)), model_rnn, [(x_train_rnn, y_train_rnn)], opt_rnn)\n",
    "    end\n",
    "    \n",
    "    Flux.reset!(model_rnn)\n",
    "    final_loss = mean(abs2.(model_rnn(x_train_rnn) .- y_train_rnn))\n",
    "    println(\"Final training loss: \", final_loss)\n",
    "    Flux.reset!(model_rnn)\n",
    "    x_plot_rnn = reshape(hcat(x_plot...), 1, :, 1)\n",
    "    y_pred_rnn = model_rnn(x_plot_rnn)\n",
    "\n",
    "    plot(x_plot, y_true, label=\"True Function\", lw=2)\n",
    "    scatter!(vec(x_train), vec(y_train), label=\"Training Data\")\n",
    "    plot!(x_plot, vec(y_pred_rnn), label=\"RNN with $(hidden_size) units\", lw=2)\n",
    "    savefig(\"Q3_rnn_$(hidden_size).png\")\n",
    "end\n",
    "\n",
    "println(\"RNN experiments complete\")\n",
    "\n",
    "data = [(x_train, y_train)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
